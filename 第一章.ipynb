{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理１００本ノック\n",
    "4/1/2021 \n",
    "第一章：https://nlp100.github.io/ja/ch01.html\n",
    "\n",
    "#### 目次\n",
    "* 00. 文字列の逆順\n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．\n",
    "* 01. 「パタトクカシーー」\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．\n",
    "* 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．\n",
    "\n",
    "* 03. 円周率\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．\n",
    "* 04. 元素記号\n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．\n",
    "* 05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．\n",
    "* 06. 集合\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．\n",
    "* 07. テンプレートによる文生成\n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．\n",
    "* 08. 暗号文\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "英小文字ならば(219 - 文字コード)の文字に置換\n",
    "その他の文字はそのまま出力\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n",
    "* 09. Typoglycemia\n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "00. 文字列の逆順\n",
    "##文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "x = 'stressed'\n",
    "r_x = ''\n",
    "for i in range(0, len(x)):\n",
    "    r_x = r_x + x[len(x)-i-1]\n",
    "print(r_x)\n",
    "\n",
    "## For any words\n",
    "word = str(input(\"Enter any word here: \"))\n",
    "r_x = ''\n",
    "for i in range(0, len(word)):\n",
    "    r_x = r_x + word[len(word)-i-1]\n",
    "print(r_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "x = 'stressed'\n",
    "print(x[::-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. 「パタトクカシーー」Permalink\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パトカー\n"
     ]
    }
   ],
   "source": [
    "x = \"パタトクカシーー\"\n",
    "new_x = \"\"\n",
    "for i in range (0, len(x)):\n",
    "    if (i%2==0):\n",
    "        new_x += x[i]\n",
    "    \n",
    "print(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "x = \"パタトクカシーー\"\n",
    "print(x[::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "x = \"パトカー\"\n",
    "y = \"タクシー\" \n",
    "new_xy = \"\"\n",
    "for i in range (0, len(x)):\n",
    "    new_xy += x[i]+y[i]\n",
    "print(new_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "x = \"パトカー\"\n",
    "y = \"タクシー\" \n",
    "print(''.join([i + j for i, j in zip(x, y)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. 円周率\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "x = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "#replace ',' and '.' \n",
    "x = x.replace(',','')\n",
    "x = x.replace('.','')\n",
    "\n",
    "word_list = x.split(' ') #use split for getting words list\n",
    "char_size = [] #make empty list\n",
    "for i in word_list: \n",
    "    char_size.append(len(i))\n",
    "print(char_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "x = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "x = x.replace(',','').replace('.','')\n",
    "print([len(w) for w in x.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04. 元素記号\n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'H', 2: 'He', 3: 'Li', 4: 'Be', 5: 'B', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 10: 'Ne', 11: 'Na', 12: 'Mi', 13: 'Al', 14: 'Si', 15: 'P', 16: 'S', 17: 'Cl', 18: 'Ar', 19: 'K', 20: 'Ca'}\n"
     ]
    }
   ],
   "source": [
    "x = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
    "#remove , and .\n",
    "x = x.replace('.','')\n",
    "x = x.replace(',','')\n",
    "#make word list\n",
    "word_list = x.split(' ')\n",
    "word_dict = {}\n",
    "num = 0\n",
    "for i in word_list:\n",
    "    num += 1\n",
    "    if num in [1, 5, 6, 7, 8, 9, 15, 16, 19]:\n",
    "        word_dict[num]=i[0]\n",
    "    else:\n",
    "        word_dict[num]=i[0:2]\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram(word): \n",
      "[['I', 'am', 'super'], ['am', 'super', 'Takumi']]\n",
      "3-gram(char): \n",
      "['Iam', 'ams', 'msu', 'sup', 'upe', 'per', 'erT', 'rTa', 'Tak', 'aku', 'kum', 'umi']\n"
     ]
    }
   ],
   "source": [
    "x = \"I am an NLPer\"\n",
    "n = 2\n",
    "def n_gram(x, n):\n",
    "    #word n-gram\n",
    "    x_word = x.split(' ')\n",
    "    word_gram = []\n",
    "    for i in range(0, len(x_word)):\n",
    "        if(i<=len(x_word)-n):\n",
    "            word_gram.append(x_word[i:i+n])\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    print(str(n)+\"-gram(word): \")\n",
    "    print(word_gram)\n",
    "    \n",
    "    #character n-gram\n",
    "    x_char = x.replace(' ','')\n",
    "    char_gram = []\n",
    "    for i in range(0, len(x_char)):\n",
    "        if(i<=len(x_char)-n): \n",
    "            char_gram.append(x_char[i:i+n])    \n",
    "            continue\n",
    "        else: \n",
    "            break\n",
    "    print(str(n)+\"-gram(char): \")\n",
    "    print(char_gram)\n",
    "    \n",
    "n_gram(\"I am super Takumi\",3)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n"
     ]
    }
   ],
   "source": [
    "def ngram(S, n):\n",
    "    r = []\n",
    "    for i in range(len(S) - n + 1):\n",
    "        r.append(S[i:i+n])\n",
    "    return r\n",
    "s = 'I am an NLPer'\n",
    "print (ngram(s.split(),2))\n",
    "print (ngram(s,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06. 集合\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_n_gram(x,n):\n",
    "    #character n-gram\n",
    "    x_char = x.replace(' ','')\n",
    "    char_gram = []\n",
    "    for i in range(0, len(x_char)):\n",
    "        if(i<=len(x_char)-n): \n",
    "            char_gram.append(x_char[i:i+n])    \n",
    "            continue\n",
    "        else: \n",
    "            break\n",
    "    return char_gram \n",
    "X = set(char_n_gram(\"paraparaparadise\",2)) #sub to X with set method \n",
    "Y = set(char_n_gram(\"paragraph\",2)) #sub to Y with set method \n",
    "\n",
    "#Union Set(和集合)\n",
    "print(X.union(Y))\n",
    "#Intersection Set(積集合)\n",
    "print(X.intersection(Y))\n",
    "#Symmetric Difference Set(差集合)\n",
    "print(X.symmetric_difference(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef ngram(S, n):\n",
    "    r = []\n",
    "    for i in range(len(S) - n + 1):\n",
    "        r.append(S[i:i+n])\n",
    "    return r\n",
    "s1 = 'paraparaparadise'\n",
    "s2 = 'paragraph'\n",
    "st1 = set(ngram(s1, 2))\n",
    "st2 = set(ngram(s2, 2))\n",
    "print(st1 | st2)\n",
    "print(st1 & st2)\n",
    "print(st1 - st2)\n",
    "print('se' in st1)\n",
    "print('se' in st2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. テンプレートによる文生成 引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def sentence(x, y, z):\n",
    "    print(str(x)+\"時の\"+str(y)+\"は\"+str(z)) \n",
    "    \n",
    "x=12\n",
    "y=\"気温\"\n",
    "z=22.4\n",
    "sentence(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 暗号文\n",
    "\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ． \n",
    "* 英小文字ならば(219 - 文字コード)の文字に置換 \n",
    "*その他の文字はそのまま出力 \n",
    "\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "暗号化: gsv jfrxp yildm ulc qfnkh levi gsv ozab wlt\n",
      "復号化: the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "def cipher(str):\n",
    "    rep = [chr(219 - ord(x)) if x.islower() else x for x in str]\n",
    "    return ''.join(rep)\n",
    "\n",
    "message = 'the quick brown fox jumps over the lazy dog'\n",
    "message = cipher(message)\n",
    "print('暗号化:', message)\n",
    "message = cipher(message)\n",
    "print('復号化:', message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Typoglycemia スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I co’udlnt beviele that I could acultlay udnentarsd what I was readnig  the pmhenaonel poewr of the haumn mind  \n"
     ]
    }
   ],
   "source": [
    "def random_gen_sent(x):\n",
    "    import random \n",
    "    \n",
    "    def rand_ints_nodup(a, b, k): #for just genelate random list\n",
    "        ns = []\n",
    "        while len(ns) < k:\n",
    "            n = random.randint(a, b)\n",
    "            if not n in ns:\n",
    "                ns.append(n)\n",
    "        return ns\n",
    "\n",
    "    #replace , . : to ''\n",
    "    x = x.replace(':','').replace('.','').replace(',','')\n",
    "    #get the words list \n",
    "    x_word = x.split(' ')\n",
    "    gene_sent = \"\" #make empty\n",
    "    new_word = \"\" #make empty\n",
    "    \n",
    "    for i in x_word:\n",
    "        if(len(i)>4):\n",
    "            new_word = \"\"\n",
    "            new_word += i[0] \n",
    "            #random with rand_ints_nodup function \n",
    "            randlist = rand_ints_nodup(1,len(i)-2,len(i)-2) #genelate random list between x[1] and x[len(x)-1]\n",
    "            for rand in randlist:\n",
    "                new_word +=i[rand]\n",
    "            new_word += i[len(i)-1]\n",
    "            gene_sent+=new_word #add the word to gene_sent\n",
    "        else: \n",
    "            gene_sent+=i #add the word to gene_sent\n",
    "        \n",
    "        gene_sent+=\" \" #add space\n",
    "\n",
    "    print(gene_sent)\n",
    "    \n",
    "x = \"I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "random_gen_sent(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-7a98e56755c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "s = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\n",
    "ans = []\n",
    "text = s.split()\n",
    "for word in text:\n",
    "    if (len(word)>4):\n",
    "        mid = list(word[1:-1])\n",
    "        random.shuffle(mid)\n",
    "        word = word[0] + ''.join(mid) + word[-1]\n",
    "        ans.append(word)\n",
    "    else:\n",
    "        ans.append(word)\n",
    "print (' '.join(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
